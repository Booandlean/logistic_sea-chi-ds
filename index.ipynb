{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Logistic Regression"]}, {"cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}, "source": ["## Learning goals\n", "\n", "1. Compare predicting a continuous outcome to predicting a class\n", "2. Compare linear to logistic regression as classification models\n", "3. Understand how the sigmoid function translates the linear equation to a probability\n", "4. Describe why logistic regression is a descriminative, parametric algorithm\n", "5. Learn how to interpret a trained logistic model's coefficients\n", "6. Explore the C (inverse regularization) paramater and hyperparameter tune\n", "7. Learn how to adjust the threshold of a logistic model\n", "8. Describe the assumptions of linear regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Why logistic 1st of our classifiers?\n", "\n", "Logistic regression is a good model to usher us into the world of classification. It takes a concept we are familiar with, a linear equation, and translates it into a form fit for predicting a class.  \n", "\n", "It generally can't compete with the best supervised learning algorithms, but it is simple, fast, and interpretable.  \n", "\n", "As we will see in mod 4, it will also serve as a segue into our lessons on neural nets."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Compare predicting a continuous outcome to predicting a class\n", "\n", "Thus far, we have worked to predict continuous target variables using linear regression. \n", "\n", "  - Continous target variables:\n", "        - Sales price of a home\n", "        - MPG of a car\n", "        - Price of Google stock\n", "        - Number of rides per day on the El\n", "        - HS graduation rates\n", "        \n", "We will now transition into another category of prediction: classification. Instead of continous target variables, we will be predicting whether records from are data are labeled as a particular class.  Whereas the output for the linear regression model can be any number, the output of our classification algorithms can only be a value designated by a set of discrete outcomes.\n", "\n", "  - Categorical target variables:\n", "        - Whether an employee will stay at a company or leave (churn)\n", "        - Whether a tumor is cancerous or benign\n", "        - Whether a flower is a rose, a dandelion, a tulip, or a daffodil\n", "        - Whether a voter is Republican, Democrat, or Independent\n", "        \n", "What are some other categorical target variables can you think of?\n", "\n", "![discuss](https://media.giphy.com/media/l0MYIAUWRmVVzfHag/giphy.gif)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### We are still dealing with **labeled data**.\n", "\n", "![labels](https://media.giphy.com/media/26Ff5evMweBsENWqk/giphy.gif)\n", "\n", "\n", "This is still supervised learning. \n", "\n", "But now, instead of the label being a continuous value, such as house price, the label is the category.  This can be either binary or multiclass.  But we still need the labels to train our models.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Compare linear to logistic regression as classification models\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The goal of logistic regression, and any classification problem, is to build a model which accurately separates the classes based on independent variables.  \n", "\n", "We are already familiar with how linear regression finds a best-fit \"line\".  It uses the MSE cost function to minimize the difference between true and predicted values.  \n", "\n", "A natural thought would be to use that \"line\" to descriminate between classes: Everything with an output greater than a certain point is classified as a 1, everything below is classified as a 0."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Glass Data\n", "Take the glass data set from the UCI Machine Learning Dataset.  \n", "\n", "It is composed of a set of features describing the physical makeup of different glass types.  \n", "\n", "Glass types 1,2,3 represent window glass.\n", "Glass types 4,5,6 represent household glass.\n", "\n", "We will try to predict whether a record is window glass or household glass. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's look at the relationship between aluminum content and glass type. \n", "There appears to be a relationship between the two, where more aluminum correlates with household glass. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["We could fit a linear regression model to this data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## What are some issues with the graph above?\n", "\n", "![talk amongst yourselves](https://media.giphy.com/media/3o6Zt44rlujPePNVVC/giphy.gif)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We predict the 0 class for lower values of al, and the 1 class for higher values of al. What's our cutoff value? Around al=2, because that's where the linear regression line crosses the midpoint between predicting class 0 and class 1.\n", "\n", "Therefore, we'll say that if household_pred >= 0.5, we predict a class of 1, else we predict a class of 0.\n", "\n", "If al=3, what class do we predict for household?\n", "\n", "If al=1.5, what class do we predict for household?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## How about we use logistic logistic regression instead?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Which performs a similar threshold decision"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Not only do we have class predictions:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Understand how the sigmoid function translates the linear equation to a probability"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Predict Proba\n", "\n", "Let's take a closer look into what fitting the logistic model results in."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Along with the class predictions, we have probabilities associated with each record:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Probabilities and the sigmoid functions\n", "Here, we can start digging deeper into how logistic regression works.\n", "\n", "The idea behind logistic regression is to model the conditional probability of a class given a set of independent features."]}, {"cell_type": "markdown", "metadata": {}, "source": ["For the binary case, it is the probability of a 0 or 1 based on a set of independent features X.\n", "\n", "$\\Large P(G = 1|X = x)$\n", "\n", "Since the total probability must be equal to 1:\n", "\n", "$\\Large P(G = 0|X = x) = 1 - P(G = 1|X = x)$ "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In order to realize such a goal, we have to somehow translate our linear output into a probability.  As we know, probability takes on a value between 0 and 1,  whereas the linear equation can output any value from $-\\infty$ to $\\infty$.\n", "\n", "![sigmoid](https://media.giphy.com/media/GtKtQ9Gb064uY/giphy.gif)\n", "\n", "In comes the sigmoid function to the rescue.\n", "\n", "\n", "<img src='https://cdn-images-1.medium.com/max/1600/1*RqXFpiNGwdiKBWyLJc_E7g.png' />\n", "\n", "If \u2018Z\u2019 goes to infinity, Y(predicted) will become 1 and if \u2018Z\u2019 goes to negative infinity, Y(predicted) will become 0.\n", "\n", "\n", "\n", "Using the sigmoid function above, if X = 1, the estimated probability would be 0.8. This tells that there is 80% chance that this observation would fall in the positive class.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In your head, work through the approximate output of the function for:\n", "  - z = 0\n", "  - z = 1000\n", "  - z = -1000"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Linking it to the linear equation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now for the fun part.  The input of the sigmoid is our trusty old linear equation.\n", "\n", "$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2, x_2 +\\ldots + \\hat\\beta_n x_n $$\n", "\n", "The linear equation is passed into the sigmoid function to produce a probability between 0 and 1\n", "$$\\displaystyle\\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n)}}$$\n", "\n", "Remember, the goal of logistic regression is to model the conditional of a class using a transformation of the linear equation.\n", "\n", "In other words:\n", "\n", "$$\\Large P(G = 1|X = x_1, x_2...x_n) = \\frac{1}{1+e^{-(\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n)}}$$\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, with some arithmetic:\n", "\n", "You can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n)}$\n", "\n", "\n", "$$ \\Large P(G = 1|X = x) = \\displaystyle \\frac{e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}$$\n", "\n", "As a result, you can compute:\n", "\n", "$$ \\Large P(G = 0|X =x) = 1- \\displaystyle \\frac{e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n}}$$\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Odds ratio\n", "\n", "This doesn't seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the *odds*, which equal p/(1-p):\n", "\n", "$$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n", "\n", "$$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$\n", "\n", "Examples:\n", "\n", "- Dice roll of 1: probability = 1/6, odds = 1/5\n", "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n", "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2\n", "\n", "$$odds = \\frac {probability} {1 - probability}$$\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$ \\Large \\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)} = e^{\\hat \\beta_o+\\hat \\beta_1 x_1 + \\hat \\beta_2 x_2...\\hat\\beta_n x_n} $$\n", "\n", "This expression can be interpreted as the *odds in favor of class 1*.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Taking the log of both sides leads to:\n", "<br><br>\n", "    $\\ln{\\dfrac{ P(G = 1|X = x) }{P(G = 0|X =x)}} = \\beta_0 + \\beta_1*X_1 + \\beta_2*X_2...\\beta_n*X_n$\n", "    \n", "Here me can see why we call it logisitic regression.\n", "\n", "Our linear function calculates the log of the probability we predict 1, divided by the probability of predicting 0.  In other words, the linear equation is calculating the **log of the odds** that we predict a class of 1.\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pair\n", "### Those are a lot of formulas to take in.  \n", "\n", "To help us reinforce how logistic regression works, let's do an exercise where we reproduce the predicted probabilities by using our coefficients.  Below is model we fit above, predicting whether glass was window or household glass.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Just like linear regression, logistic regression calculates parameters associated with features and intercept. In the fit model above, we have one coefficient associated with aluminum, and one associate with the intercept.\n", "\n", "In the cell below, use those coefficients along with the original data to calculate an array repressenting the logodds."]}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [{"data": {"text/plain": ["id\n", "22    -6.501280\n", "185   -6.292259\n", "40    -5.748805\n", "39    -5.748805\n", "51    -5.581589\n", "184   -5.372568\n", "110   -5.372568\n", "158   -5.288960\n", "153   -4.996331\n", "104   -4.954527\n", "Name: al, dtype: float64"]}, "execution_count": 67, "metadata": {}, "output_type": "execute_result"}], "source": ["log_odds = glass['al']*logreg.coef_[0] + logreg.intercept_\n", "log_odds[:10]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, take that array, and feed it into the sigmoid function to get the probabilities of class 1."]}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [{"data": {"text/plain": ["[0.0014992652045145013,\n", " 0.001847156298978947,\n", " 0.0031764633049502795,\n", " 0.0031764633049502795,\n", " 0.003752442511807795,\n", " 0.004620743778464678,\n", " 0.004620743778464678,\n", " 0.00502166268078726,\n", " 0.006717287268443344,\n", " 0.007002041892247891]"]}, "execution_count": 68, "metadata": {}, "output_type": "execute_result"}], "source": ["p_1 = [sigmoid(log_odd) for log_odd in log_odds ]\n", "p_1[:10]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 4. Describe why logistic regression is a descriminative, parametric algorithm\n", "\n", "\n", "A decision boundary is a pretty simple concept. Logistic regression is a classification algorithm, the output should be a category: Yes/No, True/False, Red/Yellow/Orange. Our prediction function however returns a probability score between 0 and 1. A decision boundary is a threshold or tipping point that helps us decide which category to choose based on probability.\n", "\n", "Logistic regression is a parametric, discriminative model.  \n", "\n", "In other words, its decisions are made via trained parameters: our beta coefficients. The hyperplane that these coefficients define is a boundary by which we can discriminate between the classes.    \n", "\n", "![](img/decision_boundary_2.jpg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 5. Interpreting Coefficients"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What does our coefficient calculated above mean?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Interpretation:** A 1 unit increase in 'al' is associated with a 4.18 unit increase in the log-odds of 'household'."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Bottom line:** Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Note on optimizing the coefficients"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Instead of optimizing the coefficients based on mean squared error, logistic regression looks to maximize the likelihood of seeing the probabilities given the class.\n", "\n", "Because we are dealing with a binary outcome, our likelihood equation comes from the Bernouli distribution:\n", "\n", "$$ \\Large Likelihood=\\prod\\limits_{i=0}^N p_i^{y_i}(1-p_i)^{1-y_i}$$\n", "\n", "Taking the log of both sides leads to the log_likelihood equation:\n", "\n", "$$ \\Large loglikelihood = \\sum\\limits_{i=1}^N y_i\\log{p_i} + (1-y_i)\\log(1-p_i) $$\n", "\n", "The goal of MLE is to maximize log-likelihood.\n", "\n", "Or, as we are generally look for minimums, we minimize the negative loglikelihood, which is our cost function:\n", "\n", "$$ \\Large negative\\ loglikelihood = \\sum\\limits_{i=1}^N - y_i\\log{p_i} - (1-y_i)\\log(1-p_i) $$\n", "\n", "When solving for the optimal coefficients of a logistic regression model, Log-Loss is the cost function that is used.\n", "\n", "\n", "The general idea is to start with a set of betas, calculate the probabilities, calculate the log-likelihood, adjust the Betas in the direction of which gradient is heading towards higher likelihood.\n", "\n", "There is no closed form solution like the normal equation in linear regression, so we have to use stocastic gradient descent.  To do so we take the derivative of the negative loglikelihood and set it to zero to find the gradient of the loglikelihood, then update our coefficients. Just like in linear regression SGD, we use a learning rate when updating them.\n", "\n", "\n", "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n", "\n", "http://wiki.fast.ai/index.php/Log_Loss\n", "\n", "Here is a good Youtube video on MLE: https://www.youtube.com/watch?v=BfKanl1aSG0\n", "\n", "Math behind the gradient of log-likelihood is ESL section 4.4.1: https://web.stanford.edu/~hastie/ElemStatLearn//.\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 6. Hyperparameter Tuning the C Variable"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have discussed 'L1' (lasso)  and 'L2' (ridge) regularization.  If you looked at the docstring of Sklearn's Logistic Regression function, you may have noticed that we can specify different types of regularization when fitting the model via the `penalty` parameter.\n", "\n", "We can also specificy the strength of the regularization via the `C` parameter. `C` is the inverse regularization strength.  So, a low `C` means high regularization strength."]}, {"cell_type": "markdown", "metadata": {}, "source": ["How do we choose between them? We iterate over possible parameters and judge the success based on our metric of choice.  We will eventually move towards grid search, which will help us be more thorough with our tuning.  For now, we will work through how to tune our C parameter with an Ridge regularization.\n", "\n", "For now, let's judge on accuracy, which can be accessed via the `score()` method of a trained model.\n"]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([1.00e-04, 1.00e+00, 2.00e+00, ..., 9.98e+02, 9.99e+02, 1.00e+03])"]}, "execution_count": 35, "metadata": {}, "output_type": "execute_result"}], "source": ["# The parameters for C can be anything above 0.  \n", "# Set up a list of possible values to try out.\n", "# Start with numbers above 0 up to 1000\n", "c_candidates = np.linspace(1,1000,1000)\n", "c_candidates = np.insert(c_candidates, 0, .0001)\n", "c_candidates"]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": ["# Split your data into training and test data with a random state of 42 \n", "# and a test size of .25\n", "X = diabetes.drop('Outcome', axis=1)\n", "y = diabetes.Outcome\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, \n", "                                                    test_size=.25)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since we are still getting used to train test split, let's now just put aside the test set and not touch it again for this lesson.\n", "\n", "We will perform a second train test split on the train set, and hyperparameter tune our C on it."]}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [{"data": {"text/plain": ["1.0"]}, "execution_count": 45, "metadata": {}, "output_type": "execute_result"}], "source": ["# Create a for loop which runs through all of the possible values of C,\n", "# fits the model on the train set, and scores the model on test set.\n", "# Add the accuracies into a dictionary or a list, whichever you prefer\n", "# Use 'l2'\n", "\n", "c_scores = {}\n", "for c in c_candidates:\n", "    regr = LogisticRegression(penalty='l2', C=c, max_iter=10000)\n", "    regr.fit(X_train, y_train)\n", "    accuracy = regr.score(X_val, y_val)\n", "    c_scores[c] = accuracy\n", "    \n", "# the best accuracy score comes with the highest regularization\n", "best_c = max(c_scores, key=c_scores.get)\n", "best_c\n", "    "]}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1.4 0.7685185185185185\n"]}], "source": ["c_scores = {}\n", "for c in c_candidates:\n", "    regr = LogisticRegression(penalty='l2', C=c, max_iter=10000)\n", "    regr.fit(X_train, y_train)\n", "    accuracy = regr.score(X_val, y_val)\n", "    c_scores[c] = accuracy\n", "    \n", "# the best accuracy score comes with the highest regularization\n", "best_c = max(c_scores, key=c_scores.get)\n", "print(best_c, c_scores[best_c])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We improved our test R^2 from .740 to .745. Not too much gain. \n", "Sometimes hyperparameter tuning can have a large effect, sometimes not. \n", "Don't rely on hyperparameter tuning to fix your model.  \n", "Treat it as a necessary step in the process which if you are lucky, may increase the predictive power of your model.\n", "\n", "In future lessons, we will use Grid Search to automate our hyperparamater tuning, and make it more thorough.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Threshold"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Because logistic regression calculates the probability of a given class, we can easily change the threshold of what is categorized as a 1 or a 0.   "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's use our best c from above, and use predict_proba() to output probabilities."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compare the output of predict and predict proba. Write out below how the output of predict_proba is related to the predict output.  "]}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [{"data": {"text/plain": ["'The default threshold is .5. Any value in the predict_proba first column above .5 is categorized as a 0. \\nAny value above .5 in the second column is categorized as 1.'"]}, "execution_count": 51, "metadata": {}, "output_type": "execute_result"}], "source": ["\"\"\"The default threshold is .5. Any value in the predict_proba first column above .5 is categorized as a 0. \n", "Any value above .5 in the second column is categorized as 1.\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, isolate one of the columns of predict_proba, and create an area of booleans which returns True if the proba is above .4"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then, use the astype method to convert the array to integers: True will become 1, and False will become 0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["While the accuracy of the model will fall by increasing the threshold, we are protecting against a certain type of error. What type of error are we reducing? Think back to Type 1 and Type 2 errors. Why might protecting against such errors be smart in a model that deals with a life-threatening medical condition?\n"]}, {"cell_type": "code", "execution_count": 72, "metadata": {}, "outputs": [{"data": {"text/plain": ["'By increasing the threshold, we are protecting against false negatives.\\nIn the case of heart disease, we should err on the side of caution.  We would rather have a false positive \\nmistake, since the individual would still be flagged for intervention. A false negative could result in death'"]}, "execution_count": 72, "metadata": {}, "output_type": "execute_result"}], "source": ["higher_threshold = probas[:,1] > .7\n", "y_hat_lower = higher_threshold.astype(int)\n", "\n", "\"\"\"By increasing the threshold, we are protecting against false negatives.\n", "In the case of heart disease, we should err on the side of caution.  We would rather have a false positive \n", "mistake, since the individual would still be flagged for intervention. A false negative could result in death\"\"\"\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 8. Assumptions of Logistic Regression\n", "\n", "Logistic regression does not make many of the key assumptions of linear regression and general linear models that are based on ordinary least squares algorithms \u2013 particularly regarding linearity, normality, and homoscedasticity.\n", "\n", "First, logistic regression does not require a linear relationship between the dependent and independent variables.  Second, the error terms (residuals) do not need to be normally distributed.  Third, homoscedasticity is not required.  \n", "\n", "**The following assumptions still apply:**\n", "\n", "1.  Binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.\n", "\n", "2. Logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data.\n", "\n", "3. Logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other.\n", "\n", "4. Logistic regression assumes linearity of independent variables and log odds.  although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.\n", "\n", "5. Logistic regression typically requires a large sample size.  A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10)."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 4}